{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################ Setup #####################################\n",
    "\n",
    "model_name = \"model_11\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pprint\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pd.set_option(\"max_colwidth\",500)\n",
    "pd.set_option(\"display.max_rows\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Read in JSON ##############################\n",
    "import json\n",
    "\n",
    "with open('kaggle_train.json') as data_file:\n",
    "    data_test = json.load(data_file)\n",
    "\n",
    "##################### Convert json to data frame ###########################\n",
    "df = pd.io.json.json_normalize(data_test)                       #convert to data frame\n",
    "df.columns = [\"cuisine\",\"ID\",\"org_ing\"]                         #rename columns of data frame\n",
    "df = df.set_index(df[\"ID\"])                                     #set ID as index\n",
    "del df[\"ID\"]                                                    #delete previous ID\n",
    "#df[\"count_ing\"] = [len(l) for l in df[\"org_ing\"]]               #add a count ingredients column\n",
    "#df[\"count_ing\"] = pickle.load(open(\"count_ing_scaler\",\"rb\"))\\\n",
    "#.transform(np.array(df[\"count_ingredients\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## Preprocessing functions #####################\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def clean(l):\n",
    "    return [[re.sub(r\"^\\s\",r\"\",re.sub(r\"\\W\",\" \",re.sub(r\"(.*)(\\((.*)oz.(.*)\\))(.*)\",r\"\\1\\5\",i))).lower() for i in r]\\\n",
    "                         for r in l]\n",
    "\n",
    "def clean_nouns(l):\n",
    "    return [[re.sub(r\"^\\s\",r\"\",re.sub(r\"\\W\",\" \",re.sub(r\"(.*)((.*)oz.(.*)\\))(.*)\",r\"\\1\\5\",i))).lower() for i in r]\\\n",
    "                         for r in l]\n",
    "\n",
    "def make_ingredients_list(l):\n",
    "    ingredients = [item for sublist in l for item in sublist]\n",
    "    \n",
    "def remove_empty(l):\n",
    "    if type(l[0]) == list:\n",
    "        return [list(filter(None,i)) for i in l]\n",
    "    else:\n",
    "        return [i.replace(',,',',').replace(', ,',', ') for i in l]\n",
    "\n",
    "def extract_nouns(text):\n",
    "    return \" \".join([word \\\n",
    "                     for word,tag in nltk.pos_tag(text.split()) \\\n",
    "                     if(tag==\"NN\" or tag==\"NNP\" or tag==\"NNS\" or tag==\"NNPS\" or tag==\"FW\")])\n",
    "\n",
    "def extract_nouns_in_list(l):\n",
    "    df[\"noun_ing\"] = [extract_nouns(i) for i in l]\n",
    "    return df[\"noun_ing\"]\n",
    "    \n",
    "def re_process(a):\n",
    "    def re_p(text):\n",
    "        text = re.sub(r\"(.*)\\b(chicken|salt)\\b(.*)\",r\"\\2\",text)                                        #only keep keywords\n",
    "        text = re.sub(r\"(.*)\\b(leaves|large|fresh|shredded|\\\n",
    "        plain|crushed|medium|ground)\\b(.*)\",r\"\\1\\3\",text)  #remove common adjectives\n",
    "        #text = re.sub(r\"(.*)\\b(cheese|flour|milk|chilies|salt|oil\\\n",
    "        #|chicken|rice|wine|onion|beans|sugar)\\b(.*)\",r\"\\2\",text)    #only keep keywords\n",
    "        #text = re.sub(\"^(water|salt|pepper|oil|butter)$\",\"\",text)                               #remove common ingredients\n",
    "        #text = re.sub(r\"(.*)\\\n",
    "        #(ground|low fat|saturated|fresh|medium|flakes|low sodium|juice|dark|black|refried\\\n",
    "        #shredded|grated|extract|pitted|all-purpose|powder|juice|large|green|red|seedless\\\n",
    "        #blanched|sliced|crushed|wedgie|sharp|whole|wholesome|freshly|plain|and)\\\n",
    "        #\\s(.*)\",r\"\\1\\3\",text)                                                                   #remove common adjectives\n",
    "        text = re.sub(r\"(.*)(lime)(.*)\",r\"(\\1)(lemon)(\\2)\",text)                                #replace synonymes ? (lemmatize)\n",
    "        return text\n",
    "    def list_re(b):\n",
    "        return [re_p(i) for i in b]\n",
    "    def str_re(b):\n",
    "        return \",\".join([re_p(i) for i in b.split(\",\")])\n",
    "    if type(a[0])==list:\n",
    "        return a.apply(list_re)\n",
    "    else:\n",
    "        return a.apply(str_re)\n",
    "    \n",
    "def make_str_ing(a):\n",
    "    return a.apply(\",\".join)     #string ingredients\n",
    "    \n",
    "def make_und_ing(ing_list,ing_str=None):\n",
    "    if type(ing_str) != pd.core.series.Series:\n",
    "        ing_str = make_str_ing(ing_list)\n",
    "    return [i.replace(\" \",\"_\").replace(\",\",\" \") for i in ing_str]\n",
    "\n",
    "def make_dtm(a,dtm_name):\n",
    "    vect = TfidfVectorizer(input=\"content\",strip_accents=\"ascii\")\n",
    "    vect.fit(list(a))\n",
    "    pickle.dump(vect,open(model_name+\"_\"+dtm_name+\"_vect.p\",\"wb\"))\n",
    "    return vect.transform(list(a))\n",
    "\n",
    "def stem(b,stemmer=None):\n",
    "    def stemm(a):\n",
    "        if type(a) == list:\n",
    "            return [stemmer.stem(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([stemmer.stem(i) for i in a.split(\",\")])\n",
    "    def lemmatizer(a):\n",
    "        if type(a) == list:\n",
    "            return [WordNetLemmatizer().lemmatize(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([WordNetLemmatizer().lemmatize(i) for i in a.split(\",\")])\n",
    "    if stemmer==None:\n",
    "        return b.apply(lemmatizer)\n",
    "    else:\n",
    "        return b.apply(stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (39774, 3005)\n"
     ]
    }
   ],
   "source": [
    "########################### Preprocessing ###############################\n",
    "df[\"ing1\"] = df[\"org_ing\"]\n",
    "#df[\"ing1\"] = re_process(df[\"ing1\"])\n",
    "#df[\"ing1\"] = stem(df[\"ing1\"])\n",
    "#df[\"ing1\"] = clean(df[\"ing1\"])\n",
    "#df[\"ing1\"] = remove_empty(df[\"ing1\"])\n",
    "df[\"ing1\"] = make_str_ing(df[\"ing1\"])\n",
    "dtm1 = make_dtm(df[\"ing1\"],\"dtm1\")\n",
    "\n",
    "#df2 = pickle.load(open(\"df_bckup3.p\",\"rb\"))\n",
    "#df2[\"ing2\"] = df2[\"nouns_ing\"]\n",
    "#df2[\"ing2\"] = re_process(df2[\"ing2\"])\n",
    "#df2[\"ing2\"] = stem(df2[\"ing2\"],LancasterStemmer())\n",
    "#df2[\"ing2\"] = clean_nouns(df2[\"ing2\"])\n",
    "#df2[\"ing2\"] = remove_empty(df2[\"ing2\"])\n",
    "#df2[\"ing2\"] = make_und_ing(df2[\"ing2\"])\n",
    "#dtm2 = make_dtm(df2[\"ing2\"],\"dtm2\")\n",
    "\n",
    "#df[\"ing3\"] = df[\"org_ing\"]\n",
    "#df[\"ing3\"] = re_process(df[\"ing3\"])\n",
    "#df[\"ing3\"] = stem(df[\"ing3\"],LancasterStemmer())\n",
    "#df[\"ing3\"] = clean(df[\"ing3\"])\n",
    "#df[\"ing3\"] = remove_empty(df[\"ing3\"])\n",
    "#df[\"ing3\"] = make_und_ing(df[\"ing3\"])\n",
    "#dtm3 = make_dtm(df[\"ing3\"],\"dtm3\")\n",
    "\n",
    "dtm = dtm1\n",
    "#dtm = scipy.sparse.hstack((dtm1,dtm2),format='csr')\n",
    "print(type(dtm),dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### For full train data ##################################\n",
    "x_train = dtm\n",
    "y_train = np.array(df[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### Setting Training and Testing Data #####################\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n",
    "def classify(model,name):\n",
    "    start_time = time()\n",
    "    model = Pipeline([('feature_selection',LinearSVC(penalty=\"l1\",dual=False)),('classification',model)])\n",
    "    acc = cross_validation.cross_val_score(model,x_train,y_train,cv=5)\n",
    "    acc = [round(i,4)*100 for i in acc]\n",
    "    model.fit(x_train,y_train)\n",
    "    end_time = round(time()-start_time,0)\n",
    "    print(name,acc,\":\",np.mean(acc),end_time)\n",
    "    return acc,model,end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM1 [78.600000000000009, 79.430000000000007, 78.829999999999998, 78.560000000000002, 79.25] : 78.934 668.0\n",
      "NB [68.510000000000005, 69.289999999999992, 67.659999999999997, 68.230000000000004, 68.489999999999995] : 68.436 440.0\n",
      "SVM1 78.934\n"
     ]
    }
   ],
   "source": [
    "###################### Classifying single-iteration classifiers ####################\n",
    "clfs = [\n",
    "        (LinearSVC(loss='l2',penalty='l1',dual=False,tol=1e-3),\"SVM1\")\n",
    "        ,(MultinomialNB(),\"NB\"),\n",
    "        #,(LinearSVC(loss='l2',penalty='l2',dual=False,tol=1e-3),\"SVM2\")\n",
    "        #,(linear_model.LogisticRegression(C=1e5),\"Logistic Regression\")\n",
    "        ]\n",
    "\n",
    "y_train = np.array(df[\"cuisine\"])\n",
    "y_train_org = np.array(df[\"cuisine\"])\n",
    "\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a,c,end_time = classify(clf,name)\n",
    "    with open(\"log_classifiers.txt\",\"a\") as text:\n",
    "        print(model_name,name,np.mean(a),end_time,file=text,sep=\",\")\n",
    "    pickle.dump(c,open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "    if np.mean(a) > best_acc:\n",
    "        best_acc = np.mean(a)\n",
    "        best_name = name\n",
    "\n",
    "print(best_name,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################### Classifying multiple-iteration classifiers #################\n",
    "clfs = [\n",
    "        #(tree.DecisionTreeClassifier(),\"Decision Tree\")\n",
    "        #,(RandomForestClassifier(n_estimators=500,n_jobs=5),\"RF1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=3)),n_estimators=500,learning_rate=1),\"ADA 1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=500,learning_rate=1),\"ADA 2\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1000,learning_rate=1),\"ADA 3\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1400,learning_rate=1),\"ADA 4\")\n",
    "        (AdaBoostClassifier((DecisionTreeClassifier(max_depth=7)),n_estimators=100,learning_rate=1),\"ADA 5\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=9)),n_estimators=1200,learning_rate=1),\"ADA 6\")\n",
    "        ,(RandomForestClassifier(n_estimators=700,n_jobs=5),\"RF2\")\n",
    "        ]\n",
    "\n",
    "n = 8\n",
    "\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a = [0] * n\n",
    "    c = [0] * n\n",
    "    for i in range(0,n):\n",
    "        a[i],c[i] = classify(clf,name)\n",
    "        if a[i] > best_acc:\n",
    "            best_acc = a[i]\n",
    "            best_name = name\n",
    "    pickle.dump(c[a.index(max(a))],open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "\n",
    "print(best_name,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######### Percentage errors in individual cuisines reults for cv #########\n",
    "from collections import Counter\n",
    "a=Counter(c.predict(x_cv))\n",
    "b=Counter(y_cv)\n",
    "d=pd.DataFrame([(i[0],i[1],b[i[0]] - i[1],round((b[i[0]] - i[1])/i[1]*100)) for i in a.items()]).sort_values(by=3,ascending=False)\n",
    "d.columns = [\"cuisine\",\"total\",\"absolute false negatives\",\"percentage false negatives\"]\n",
    "with open(\"error_cuisines.txt\",\"w\") as text:\n",
    "    print(d,file=text)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Intro #####################################\n",
    "\n",
    "#clean : (oz.),keep alpha_num,lowercase\n",
    "#extract nouns\n",
    "#regex\n",
    "#individual dtm\n",
    "#adding of model-2 level-1 prediction feature\n",
    "\n",
    "#cv : 5-fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
