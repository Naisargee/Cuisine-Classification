{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################ Setup #####################################\n",
    "\n",
    "model_name = \"model_2_N_0\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pprint\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pd.set_option(\"max_colwidth\",500)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "df = pickle.load(open(\"df_bckup3.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################## Preprocessing functions #####################\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def clean(l):\n",
    "    return [[re.sub(r\"^\\s\",r\"\",re.sub(r\"\\W\",\" \",re.sub(r\"(.*)((.*)oz.(.*)\\))(.*)\",r\"\\1\\5\",i))).lower() for i in r]\\\n",
    "                         for r in l]\n",
    "\n",
    "def make_ingredients_list(l):\n",
    "    ingredients = [item for sublist in l for item in sublist]\n",
    "    \n",
    "def remove_empty(l):\n",
    "    if type(l[0]) == list:\n",
    "        return [list(filter(None,i)) for i in l]\n",
    "    else:\n",
    "        return [i.replace(',,',',').replace(', ,',', ') for i in l]\n",
    "\n",
    "def extract_nouns(text):\n",
    "    return \" \".join([word \\\n",
    "                     for word,tag in nltk.pos_tag(text.split()) \\\n",
    "                     if(tag==\"NN\" or tag==\"NNP\" or tag==\"NNS\" or tag==\"NNPS\" or tag==\"FW\")])\n",
    "\n",
    "def extract_nouns_in_list(l):\n",
    "    df[\"noun_ing\"] = [extract_nouns(i) for i in l]\n",
    "    return df[\"noun_ing\"]\n",
    "    \n",
    "def re_process(a):\n",
    "    def re_p(text):\n",
    "        text = re.sub(r\"(.*)\\b(chicken|salt)\\b(.*)\",r\"\\2\",text)                                        #only keep keywords\n",
    "        text = re.sub(r\"(.*)\\b(leaves|large|fresh|shredded|\\\n",
    "        plain|crushed|medium|ground)\\b(.*)\",r\"\\1\\3\",text)  #remove common adjectives\n",
    "        #text = re.sub(r\"(.*)\\b(cheese|flour|milk|chilies|salt|oil\\\n",
    "        #|chicken|rice|wine|onion|beans|sugar)\\b(.*)\",r\"\\2\",text)    #only keep keywords\n",
    "        #text = re.sub(\"^(water|salt|pepper|oil|butter)$\",\"\",text)                               #remove common ingredients\n",
    "        #text = re.sub(r\"(.*)\\\n",
    "        #(ground|low fat|saturated|fresh|medium|flakes|low sodium|juice|dark|black|refried\\\n",
    "        #shredded|grated|extract|pitted|all-purpose|powder|juice|large|green|red|seedless\\\n",
    "        #blanched|sliced|crushed|wedgie|sharp|whole|wholesome|freshly|plain|and)\\\n",
    "        #\\s(.*)\",r\"\\1\\3\",text)                                                                   #remove common adjectives\n",
    "        text = re.sub(r\"(.*)(lime)(.*)\",r\"(\\1)(lemon)(\\2)\",text)                                #replace synonymes ? (lemmatize)\n",
    "        return text\n",
    "    def list_re(b):\n",
    "        return [re_p(i) for i in b]\n",
    "    def str_re(b):\n",
    "        return \",\".join([re_p(i) for i in b.split(\",\")])\n",
    "    if type(a[0])==list:\n",
    "        return a.apply(list_re)\n",
    "    else:\n",
    "        return a.apply(str_re)\n",
    "    \n",
    "def make_str_ing(a):\n",
    "    return a.apply(\",\".join)     #string ingredients\n",
    "    \n",
    "def make_und_ing(ing_list,ing_str=None):\n",
    "    if type(ing_str) != pd.core.series.Series:\n",
    "        ing_str = make_str_ing(ing_list)\n",
    "    return [i.replace(\" \",\"_\").replace(\",\",\" \") for i in ing_str]\n",
    "\n",
    "def make_dtm(a):\n",
    "    vect = CountVectorizer(input=\"content\",strip_accents=\"ascii\",binary=True)\n",
    "    vect.fit(list(a))\n",
    "    pickle.dump(vect,open(model_name+\"_vect.p\",\"wb\"))\n",
    "    return vect.transform(list(a))\n",
    "\n",
    "def stem(b,stemmer=None):\n",
    "    def stemm(a):\n",
    "        if type(a) == list:\n",
    "            return [stemmer.stem(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([stemmer.stem(i) for i in a.split(\",\")])\n",
    "    def lemmatizer(a):\n",
    "        if type(a) == list:\n",
    "            return [WordNetLemmatizer().lemmatize(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([WordNetLemmatizer().lemmatize(i) for i in a.split(\",\")])\n",
    "    if stemmer==None:\n",
    "        return b.apply(lemmatizer)\n",
    "    else:\n",
    "        return b.apply(stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################### Preprocessing ###############################\n",
    "df = pickle.load(open(\"df_bckup3.p\",\"rb\"))\n",
    "df[\"ing\"] = re_process(df[\"nouns_ing\"])\n",
    "#df[\"ing\"] = stem(df[\"ing\"],LancasterStemmer())\n",
    "df[\"ing\"] = clean(df[\"ing\"])\n",
    "df[\"ing\"] = remove_empty(df[\"ing\"])\n",
    "df[\"ing\"] = make_und_ing(df[\"ing\"])\n",
    "\n",
    "dtm = make_dtm(df[\"ing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################### For full train data ##################################\n",
    "x_train = dtm\n",
    "y_train = np.array(df[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################### Setting Training and Testing Data #####################\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n",
    "def classify(model,name):\n",
    "    start_time = time()\n",
    "    model = Pipeline([('feature_selection',LinearSVC(penalty=\"l1\",dual=False)),('classification',model)])\n",
    "    acc = cross_validation.cross_val_score(model,x_train,y_train,cv=2)\n",
    "    acc = [round(i,4)*100 for i in acc]\n",
    "    model.fit(x_train,y_train)\n",
    "    end_time = round(time()-start_time,0)\n",
    "    print(name,acc,\":\",np.mean(acc),end_time)\n",
    "    with open(\"log_classifiers.txt\",\"a\") as text:\n",
    "        print(model_name+\",\"+name+\",\"+str(np.mean(acc))+\",\"+str(end_time),file=text)\n",
    "    return np.mean(acc),model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b04786ea899b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcombo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'combo' is not defined"
     ]
    }
   ],
   "source": [
    "#x_train = np.array(dtm)\n",
    "#y_train = np.array(df[\"cuisine\"])\n",
    "x_train.shape\n",
    "type(x_train)\n",
    "print(np.in1d(y_train,combo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################### 2-level single-iteration classifiers ####################\n",
    "clfs = [\n",
    "        (LinearSVC(loss='l2',penalty='l1',dual=False,tol=1e-3),\"SVM1\")\n",
    "        ,(LinearSVC(loss='l2',penalty='l2',dual=False,tol=1e-3),\"SVM2\")\n",
    "        ,(linear_model.LogisticRegression(C=1e5),\"Logistic Regression\")\n",
    "        ]\n",
    "\n",
    "combos = ['brazilian','british','cajun_creole','chinese',\\\n",
    "                               'filipino','greek','indian','irish',\\\n",
    "                               'jamaican','japanese','korean','mexican','moroccan','russian',\\\n",
    "                               'southern_us','spanish','thai','vietnamese']\n",
    "combos = [[i] for i in combos]\n",
    "combos.append([\"french\",\"italian\"])\n",
    "\n",
    "### Level 1 ####\n",
    "x_train_org = x_train\n",
    "y_train_org = np.array(list(y_train))\n",
    "for combo in combos:\n",
    "    new = \"_\".join(combo)\n",
    "    for cuisine in combo:\n",
    "        y_train[y_train==cuisine] = new\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a,c = classify(clf,name)\n",
    "#    pickle.dump(c,open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "    if a > best_acc:\n",
    "        best_acc = a\n",
    "        best_name = name\n",
    "print(best_name,best_acc)\n",
    "\n",
    "#### Level 2 ####\n",
    "for combo in combos:\n",
    "    if len(combo)==1:\n",
    "        continue\n",
    "    y_train = y_train_org\n",
    "    x_train = x_train_org\n",
    "    y_train = y_train[np.in1d(y_train,combo)]\n",
    "    x_train = x_train[np.in1d(y_train,combo)]\n",
    "    print(combo)\n",
    "    best_acc = 0\n",
    "    best_name = \"\"\n",
    "    for clf,name in clfs:\n",
    "        a,c = classify(clf,name+\"___\"+(\"_\".join(combo)))\n",
    "        pickle.dump(c,open(model_name+\"_\"+name+\"___\"+(\"_\".join(combo))+\".p\",\"wb\"))\n",
    "        if a > best_acc:\n",
    "            best_acc = a\n",
    "            best_name = name\n",
    "    print(best_name,best_acc,\"\\n\\n\")\n",
    "y_train = y_train_org\n",
    "x_train = x_train_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################### Classifying multiple-iteration classifiers #################\n",
    "clfs = [\n",
    "        #,(tree.DecisionTreeClassifier(),\"Decision Tree\")\n",
    "        ,(RandomForestClassifier(n_estimators=500,n_jobs=5),\"RF1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=3)),n_estimators=500,learning_rate=1),\"ADA 1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=500,learning_rate=1),\"ADA 2\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1000,learning_rate=1),\"ADA 3\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1400,learning_rate=1),\"ADA 4\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=7)),n_estimators=100,learning_rate=1),\"ADA 5\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=7)),n_estimators=300,learning_rate=1),\"ADA 6\")\n",
    "        ,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=9)),n_estimators=100,learning_rate=1),\"ADA 7\")\n",
    "        ,(RandomForestClassifier(n_estimators=700,n_jobs=5),\"RF2\")\n",
    "        ]\n",
    "\n",
    "n = 8\n",
    "\n",
    "### Level 1 ####\n",
    "x_train_org = x_train\n",
    "y_train_org = np.array(list(y_train))\n",
    "for combo in combos:\n",
    "    new = \"_\".join(combo)\n",
    "    for cuisine in combo:\n",
    "        y_train[y_train==cuisine] = new\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a = [0] * n\n",
    "    c = [0] * n\n",
    "    for i in range(0,n):\n",
    "        a[i],c[i] = classify(clf,name)\n",
    "        if a[i] > best_acc:\n",
    "            best_acc = a[i]\n",
    "            best_name = name\n",
    "    pickle.dump(c[a.index(max(a))],open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "    print(best_name,best_acc)\n",
    "\n",
    "#### Level 2 ####\n",
    "for combo in combos:\n",
    "    if len(combo)==1:\n",
    "        continue\n",
    "    y_train = y_train_org[np.in1d(y_train,combo)]\n",
    "    x_train = x_train_org[np.in1d(y_train,combo)]\n",
    "    print(combo)\n",
    "    best_acc = 0\n",
    "    best_name = \"\"\n",
    "    for clf,name in clfs:\n",
    "    a = [0] * n\n",
    "    c = [0] * n\n",
    "    for i in range(0,n):\n",
    "        a[i],c[i] = classify(clf,name+\"___\"+(\"_\".join(combo)))\n",
    "        if a[i] > best_acc:\n",
    "            best_acc = a[i]\n",
    "            best_name = name\n",
    "    pickle.dump(c[a.index(max(a))],open(model_name+\"_\"+name+\"___\"+(\"_\".join(combo))+\".p\",\"wb\"))\n",
    "    print(best_name,best_acc)\n",
    "y_train = y_train_org\n",
    "x_train = x_train_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################### make a data frame which shows similarity between cuisines ################\n",
    "common = []\n",
    "cuisines = set(y_cv)\n",
    "for i in cuisines:\n",
    "    for j in cuisines:\n",
    "        if i==j:\n",
    "            continue\n",
    "        i1 = len(list(y_cv[y_cv==i]))                          #Total tuples in original dataset\n",
    "        i2 = len((pred_cv[y_cv==i])[pred_cv[y_cv==i]==i])      #Total tuples in predicted\n",
    "        i3 = 100 - round(i2/i1 * 100)                          #% inaccuracy\n",
    "        i4 = len((pred_cv[y_cv==i])[pred_cv[y_cv==i]==j])      #Actually in cuisine 1 but predicted in cuisine 2\n",
    "        common.append([i,j,i1,i2,i3,i4])\n",
    "common = pd.DataFrame(common)\n",
    "common.columns = [\"cuisine 1\",\"cuisine 2\",\"total\",\"pred\",\"%\",\"v\"]\n",
    "common = common.sort_values(by=\"v\",ascending=False)\n",
    "with open(\"cuisines_similarity.txt\",\"w\") as text:\n",
    "    print(common,file=text)\n",
    "    \n",
    "common = pd.pivot_table(common,values = \"v\",\\\n",
    "                        columns=[\"cuisine 1\",\"total\",\"pred\",\"%\"], index = \"cuisine 2\")\n",
    "with open(\"cuisines_similarity.txt\",\"a\") as text:\n",
    "    print(common,file=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test.txt\") as text:\n",
    "\n",
    "    print(common[[\"cuisine 1\",\"cuisine 2\"]][common[\"v\"]<],file=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#common[\"v\"].plot()\n",
    "#plt.show()\n",
    "##### ########\n",
    "(set(y_train_org))\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = np.array([1,2,2,3,2,3,4,5])\n",
    "i1 = np.array(i)\n",
    "i1[i1==2] = 100\n",
    "i==i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Intro #####################################\n",
    "\n",
    "#clean : (oz.),keep alpha_num,lowercase\n",
    "#extract nouns\n",
    "#regex\n",
    "\n",
    "#cv : k-fold\n",
    "#classification : 2-level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
