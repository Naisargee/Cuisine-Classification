{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Read in JSON ###########################\n",
    "import json\n",
    "\n",
    "with open('kaggle_train.json') as data_file:\n",
    "    data_train = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################### Convert json to data frame #######################\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.io.json.json_normalize(data_train)    #convert to data frame\n",
    "df.columns = [\"cuisine\",\"ID\",\"ingredients\"]   #rename columns of data frame\n",
    "#df = df.sort(columns=\"ID\")                    #sort by ID ?\n",
    "df = df.set_index(df[\"ID\"])                   #set ID as index\n",
    "del df[\"ID\"]                                  #delete previous ID\n",
    "#df[\"cuisine\"] = df[\"cuisine\"].astype(\"category\")   #categorize cuisines ?\n",
    "df[\"count_ingredients\"] = [len(l) for l in df[\"ingredients\"]]  #add a count ingredients columns\n",
    "#df.groupby(\"cuisine\").count()\n",
    "#print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim in a bal\n"
     ]
    }
   ],
   "source": [
    "############################## Stemming and Lematizing ###################\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "def stemmer(text,stemmer=LancasterStemmer()):\n",
    "    return stemmer.stem(text).lower()\n",
    "\n",
    "\n",
    "    \n",
    "df[\"ingredients\"] = [[\" \".join(stemmer(word) for word in x.split()) for x in l] for l in df[\"ingredients\"]]  #stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(u'brazilian', {'boxes': [<matplotlib.lines.Line2D object at 0x000000000DF5A898>], 'fliers': [<matplotlib.lines.Line2D object at 0x000000000DF72470>], 'medians': [<matplotlib.lines.Line2D object at 0x000000000DF66EB8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x000000000DF5A9E8>, <matplotlib.lines.Line2D object at 0x000000000DF5AE10>], 'caps': [<matplotlib.lines.Line2D object at 0x000000000DF663C8>, <matplotlib.lines.Line2D object at 0x000000000DF66940>]}), (u'british', {'boxes': [<matplotlib.lines.Line2D object at 0x000000000DF72CF8>], 'fliers': [<matplotlib.lines.Line2D object at 0x000000002972C9E8>], 'medians': [<matplotlib.lines.Line2D object at 0x000000002972C470>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x000000000DF72F60>, <matplotlib.lines.Line2D object at 0x00000000297213C8>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000029721940>, <matplotlib.lines.Line2D object at 0x0000000029721EB8>]}), (u'cajun_creole', {'boxes': [<matplotlib.lines.Line2D object at 0x00000000297382B0>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000029745F28>], 'medians': [<matplotlib.lines.Line2D object at 0x00000000297459B0>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x00000000297384E0>, <matplotlib.lines.Line2D object at 0x0000000029738908>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000029738E80>, <matplotlib.lines.Line2D object at 0x0000000029745438>]}), (u'chinese', {'boxes': [<matplotlib.lines.Line2D object at 0x00000000297527F0>], 'fliers': [<matplotlib.lines.Line2D object at 0x00000000295DE4A8>], 'medians': [<matplotlib.lines.Line2D object at 0x00000000295D3EF0>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000029752A20>, <matplotlib.lines.Line2D object at 0x0000000029752E48>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000295D3400>, <matplotlib.lines.Line2D object at 0x00000000295D3978>]}), (u'filipino', {'boxes': [<matplotlib.lines.Line2D object at 0x00000000295DED30>], 'fliers': [<matplotlib.lines.Line2D object at 0x00000000295F8BA8>], 'medians': [<matplotlib.lines.Line2D object at 0x00000000295F8630>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x00000000295DEF60>, <matplotlib.lines.Line2D object at 0x00000000295EB588>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000295EBB00>, <matplotlib.lines.Line2D object at 0x00000000295F80B8>]}), (u'french', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000029605470>], 'fliers': [<matplotlib.lines.Line2D object at 0x00000000047E5278>], 'medians': [<matplotlib.lines.Line2D object at 0x000000000DF523C8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x00000000296056A0>, <matplotlib.lines.Line2D object at 0x0000000029605AC8>], 'caps': [<matplotlib.lines.Line2D object at 0x000000000DF5A4E0>, <matplotlib.lines.Line2D object at 0x000000000DF52F98>]}), (u'greek', {'boxes': [<matplotlib.lines.Line2D object at 0x00000000047C7978>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000029611710>], 'medians': [<matplotlib.lines.Line2D object at 0x000000002887F7B8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x00000000047C7400>, <matplotlib.lines.Line2D object at 0x00000000047C0DD8>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000047AEAC8>, <matplotlib.lines.Line2D object at 0x000000002870F358>]}), (u'indian', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000029620080>], 'fliers': [<matplotlib.lines.Line2D object at 0x000000002962EC50>], 'medians': [<matplotlib.lines.Line2D object at 0x000000002962E6D8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000029620208>, <matplotlib.lines.Line2D object at 0x0000000029620630>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000029620BA8>, <matplotlib.lines.Line2D object at 0x000000002962E160>]}), (u'irish', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000029638518>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028C4E390>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000029647DD8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000029638748>, <matplotlib.lines.Line2D object at 0x0000000029638D30>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000296472E8>, <matplotlib.lines.Line2D object at 0x0000000029647860>]}), (u'italian', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028C4EC18>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028C698D0>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028C69358>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028C4EE48>, <matplotlib.lines.Line2D object at 0x0000000028C5D2B0>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028C5D828>, <matplotlib.lines.Line2D object at 0x0000000028C5DDA0>]}), (u'jamaican', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028C76198>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028C82E10>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028C82898>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028C763C8>, <matplotlib.lines.Line2D object at 0x0000000028C767F0>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028C76D68>, <matplotlib.lines.Line2D object at 0x0000000028C82320>]}), (u'japanese', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028C8F6D8>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028CA9390>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028C9DDD8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028C8F908>, <matplotlib.lines.Line2D object at 0x0000000028C8FD30>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028C9D2E8>, <matplotlib.lines.Line2D object at 0x0000000028C9D860>]}), (u'korean', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028CA9C18>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028CC4A90>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028CC4518>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028CA9E48>, <matplotlib.lines.Line2D object at 0x0000000028CB8470>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028CB89E8>, <matplotlib.lines.Line2D object at 0x0000000028CB8F60>]}), (u'mexican', {'boxes': [<matplotlib.lines.Line2D object at 0x00000000271F5358>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000027201FD0>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000027201A58>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x00000000271F5588>, <matplotlib.lines.Line2D object at 0x00000000271F59B0>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000271F5F28>, <matplotlib.lines.Line2D object at 0x00000000272014E0>]}), (u'moroccan', {'boxes': [<matplotlib.lines.Line2D object at 0x000000002720D898>], 'fliers': [<matplotlib.lines.Line2D object at 0x000000002722A550>], 'medians': [<matplotlib.lines.Line2D object at 0x000000002721DF98>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x000000002720DAC8>, <matplotlib.lines.Line2D object at 0x000000002720DEF0>], 'caps': [<matplotlib.lines.Line2D object at 0x000000002721D4A8>, <matplotlib.lines.Line2D object at 0x000000002721DA20>]}), (u'russian', {'boxes': [<matplotlib.lines.Line2D object at 0x000000002722ADD8>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000027243A90>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000027243518>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000027237048>, <matplotlib.lines.Line2D object at 0x0000000027237470>], 'caps': [<matplotlib.lines.Line2D object at 0x00000000272379E8>, <matplotlib.lines.Line2D object at 0x0000000027237F60>]}), (u'southern_us', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000027250358>], 'fliers': [<matplotlib.lines.Line2D object at 0x00000000272681D0>], 'medians': [<matplotlib.lines.Line2D object at 0x000000002725CC18>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000027250588>, <matplotlib.lines.Line2D object at 0x0000000027250B70>], 'caps': [<matplotlib.lines.Line2D object at 0x000000002725C128>, <matplotlib.lines.Line2D object at 0x000000002725C6A0>]}), (u'spanish', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000027268A58>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028EE4710>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028EE4198>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000027268C88>, <matplotlib.lines.Line2D object at 0x0000000028ED90F0>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028ED9668>, <matplotlib.lines.Line2D object at 0x0000000028ED9BE0>]}), (u'thai', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028EF1080>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028EFDC50>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028EFD6D8>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028EF1208>, <matplotlib.lines.Line2D object at 0x0000000028EF1630>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028EF1BA8>, <matplotlib.lines.Line2D object at 0x0000000028EFD160>]}), (u'vietnamese', {'boxes': [<matplotlib.lines.Line2D object at 0x0000000028F0B518>], 'fliers': [<matplotlib.lines.Line2D object at 0x0000000028F671D0>], 'medians': [<matplotlib.lines.Line2D object at 0x0000000028F5BC18>], 'means': [], 'whiskers': [<matplotlib.lines.Line2D object at 0x0000000028F0B748>, <matplotlib.lines.Line2D object at 0x0000000028F0BB70>], 'caps': [<matplotlib.lines.Line2D object at 0x0000000028F5B128>, <matplotlib.lines.Line2D object at 0x0000000028F5B6A0>]})])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"cuisine\").boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\Anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-b608342c9c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Maximum in a baLl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mextract_nouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-b608342c9c1f>\u001b[0m in \u001b[0;36mextract_nouns\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_nouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m                      \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                      \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NN\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NNP\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NNS\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NNPS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Maximum in a baLl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m--> 101\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    102\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\Anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def extract_nouns(text):\n",
    "    return \" \".join([word \\\n",
    "                     for word,tag in nltk.pos_tag(nltk.word_tokenize(text)) \\\n",
    "                     if(tag==\"NN\" or tag==\"NNP\" or tag==\"NNS\" or tag==\"NNPS\")])\n",
    "\n",
    "st = \"Maximum in a baLl\"\n",
    "\n",
    "#print extract_nouns(st)\n",
    "\n",
    "st = \" \".join([stemmer(w).lower() for w in st.split()])\n",
    "print st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################### Store the precious data frame #############################\n",
    "import pickle\n",
    "pickle.dump(df,open(\"data_train_data_frame.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'black olives', u'grape tomatoes', u'garlic', u'pepper', u'purple onion', u'seasoning', u'garbanzo beans', u'feta cheese crumbles', u'plain flour']\n",
      "[u'sweetened coconut', u'baking chocolate', u'egg roll wrappers', u'bottled low sodium salsa', u'vegan parmesan cheese', u'clam sauce', u'mahlab', u'(10 oz.) frozen chopped spinach, thawed and squeezed dry', u'figs']\n",
      "39774 <type 'list'> <type 'dict'> {u'cuisine': u'southern_us', u'id': 25693, u'ingredients': [u'plain flour', u'ground pepper', u'salt', u'tomatoes', u'ground black pepper', u'thyme', u'eggs', u'green tomatoes', u'yellow corn meal', u'milk', u'vegetable oil']}\n",
      "6714 <type 'dict'>\n",
      "20 <type 'dict'>\n",
      "{u'irish': 0, u'mexican': 1, u'chinese': 2, u'filipino': 3, u'vietnamese': 4, u'spanish': 13, u'japanese': 7, u'moroccan': 5, u'french': 12, u'greek': 9, u'indian': 10, u'jamaican': 11, u'british': 8, u'brazilian': 6, u'russian': 14, u'cajun_creole': 15, u'thai': 16, u'southern_us': 17, u'korean': 18, u'italian': 19}\n"
     ]
    }
   ],
   "source": [
    "################################ Make Ingredients and cuisines lists and dictionaries ##################################\n",
    "data = data_train\n",
    "X = []\n",
    "Y = []\n",
    "for k in data:\n",
    "    X.extend(list(k[\"ingredients\"]))\n",
    "    Y.append(k[\"cuisine\"])\n",
    "print X[1:10]\n",
    "\n",
    "X=set(X)\n",
    "Y=set(Y)\n",
    "\n",
    "print list(X)[1:10]\n",
    "ingredients={}\n",
    "cuisine={}\n",
    "k=0\n",
    "for x in X:\n",
    "    ingredients[x]=k\n",
    "    k=k+1\n",
    "k=0\n",
    "for y in Y:\n",
    "    cuisine[y]=k\n",
    "    k=k+1\n",
    "print(len(data)),type(data),type(data[1]),data[1]\n",
    "print(len(ingredients)),type(ingredients)\n",
    "print(len(cuisine)),type(cuisine)\n",
    "print(cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yyyyyyyyyyyyy\n",
      "27841\n",
      "6714\n",
      "6714\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##########Setting Training and Testing Data####################################\n",
    "###############################################################################\n",
    "import pickle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "#data = data[:100]\n",
    "\n",
    "data_train,data_test=train_test_split(data,test_size=0.3,random_state=1234)\n",
    "data_cv,data_test=train_test_split(data_test,test_size=0.5,random_state=1234)\n",
    "print(\"yyyyyyyyyyyyy\")\n",
    "print(len(data_train))\n",
    "features_train=[]\n",
    "label_train=[]\n",
    "\n",
    "for k in data_train:\n",
    "    x=[0 for x in range(0,len(ingredients))]\n",
    "    for k2 in k[\"ingredients\"]:\n",
    "        x[ingredients[k2]]=1\n",
    "    features_train.append(x)\n",
    "    label_train.append(cuisine[k[\"cuisine\"]])\n",
    "\n",
    "features_cv=[]\n",
    "label_cv=[]\n",
    "\n",
    "for k in data_cv:\n",
    "    x=[0 for x in range(0,len(ingredients))]\n",
    "    for k2 in k[\"ingredients\"]:\n",
    "        x[ingredients[k2]]=1\n",
    "    features_cv.append(x)\n",
    "    label_cv.append(cuisine[k[\"cuisine\"]])\n",
    "\n",
    "features_test=[]\n",
    "label_test=[]\n",
    "\n",
    "for k in data_test:\n",
    "    x=[0 for x in range(0,len(ingredients))]\n",
    "    for k2 in k[\"ingredients\"]:\n",
    "        x[ingredients[k2]]=1\n",
    "    features_test.append(x)\n",
    "    label_test.append(cuisine[k[\"cuisine\"]])\n",
    "\n",
    "print(len(features_train[0]))\n",
    "print(len(features_cv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = PCA(n_components=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = p.fit_transform(features_train)\n",
    "pickle.dump(p,open(\"PCA_object.p\",\"wb\"))\n",
    "pickle.dump(p,open(\"PCA_x_train.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-daa3dcb79d63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PCA_object.p\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PCA_x_train.p\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\pickle.pyc\u001b[0m in \u001b[0;36mload_eof\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_eof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = pickle.load(open(\"PCA_object.p\",\"rb\"))\n",
    "x_train = pickle.load(open(\"PCA_x_train.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_cv = p.transform(features_cv)\n",
    "x_test = p.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##########Setting Training and Testing Data####################################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def classify(model,name):\n",
    "    model = Pipeline([('feature_selection',LinearSVC(penalty=\"l1\",dual=False)),('classification',model)])\n",
    "    model.fit(features_train,label_train)\n",
    "#    print(\"Fitted\")\n",
    "#    predicted= model.predict(features_cv)\n",
    "    acc = round(model.score(features_cv,label_cv),4)*100\n",
    "    print(\"Accuracy for \",name,\" : \",acc)\n",
    "    file_k.write(str(name)+\",\"+str(acc)+\"\\n\")\n",
    "    return acc,clf\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = [\n",
    "        #(AdaBoostClassifier((DecisionTreeClassifier(max_depth=3)),n_estimators=100,learning_rate=1),\"ADA 1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=500,learning_rate=1),\"ADA 2\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1000,learning_rate=1),\"ADA 3\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1400,learning_rate=1),\"ADA 4\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=7)),n_estimators=1200,learning_rate=1),\"ADA 5\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=9)),n_estimators=1200,learning_rate=1),\"ADA 6\")\n",
    "        #,(tree.DecisionTreeClassifier(),\"Decision Tree\")\n",
    "        #,(RandomForestClassifier(n_estimators=500,n_jobs=20),\"Random Forests 1\")\n",
    "        (linear_model.LogisticRegression(C=1e5),\"Logistic Regression\")\n",
    "        #,(LinearSVC(loss='l2',penalty='l1',dual=False,tol=1e-3),\"SVM1\")\n",
    "        #,(LinearSVC(loss='l2',penalty='l2',dual=False,tol=1e-3),\"SVM2\")\n",
    "        ]\n",
    "#model.fit(features,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression_.txt\n"
     ]
    }
   ],
   "source": [
    "file_log_classifiers_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    file_log_classifiers_name += name+\"_\"\n",
    "file_log_classifiers_name += \".txt\"\n",
    "file_k=open(file_log_classifiers_name,\"w\")\n",
    "print file_log_classifiers_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1\n",
      "('Accuracy for ', 'Logistic Regression', ' : ', 63.41)\n",
      "best clf: Logistic Regression\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now on with testing!! The moment of truth! Dil thaam ke bethiye.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 6714 features per sample; expecting 4752",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-47e120d58c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best_classifier_name.p\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#predicted= clf.predict(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy for \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mfile_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nBest classifier : \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \"\"\"\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \"\"\"\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 204\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 6714 features per sample; expecting 4752"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(1,2):\n",
    "    print \"Episode :\",i\n",
    "    file_k.write(\"\\n\\n\\nIteration : \"+str(i)+\"\\n\\n\")\n",
    "    for clf,name in clfs:\n",
    "        a,c = classify(clf,name)\n",
    "        if a>acc:\n",
    "            print \"best clf:\",name\n",
    "            pickle.dump(c,open(\"best_classifier.p\",\"wb\"))\n",
    "            pickle.dump(name,open(\"best_classifier_name.p\",\"wb\"))\n",
    "    print(\"\\n\\n\\n\")\n",
    "file_k.close()\n",
    "\n",
    "print(\"Now on with testing!! The moment of truth! Dil thaam ke bethiye.\")\n",
    "\n",
    "clf = pickle.load(open(\"best_classifier.p\",\"rb\"))\n",
    "name = pickle.load(open(\"best_classifier_name.p\",\"rb\"))\n",
    "#predicted= clf.predict(x)\n",
    "acc = round(clf.score(features_test,label_test),4)*100\n",
    "print(\"Accuracy for \",name,\" : \",acc)\n",
    "file_k.write(\"\\nBest classifier : \"+str(name)+':'+str(acc))\n",
    "#predicted= model.predict(test_data)\n",
    "#for k in predicted:\n",
    "#    print(k, end=\" \")\n",
    "#print()\n",
    "#print(\"Prediction Score\")\n",
    "#print()\n",
    "file_k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
