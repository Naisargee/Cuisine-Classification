{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################ Setup #####################################\n",
    "\n",
    "model_name = \"model_1_2\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pprint\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pd.set_option(\"max_colwidth\",500)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "\n",
    "df = pickle.load(open(\"df_bckup3.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################## Preprocessing functions #####################\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def clean(l):\n",
    "    return [[re.sub(r\"^\\s\",r\"\",re.sub(r\"\\W\",\" \",re.sub(r\"(.*)((.*)oz.(.*)\\))(.*)\",r\"\\1\\5\",i))).lower() for i in r]\\\n",
    "                         for r in l]\n",
    "\n",
    "def make_ingredients_list(l):\n",
    "    ingredients = [item for sublist in l for item in sublist]\n",
    "    \n",
    "def remove_empty(l):\n",
    "    if type(l[0]) == list:\n",
    "        return [list(filter(None,i)) for i in l]\n",
    "    else:\n",
    "        return [i.replace(',,',',').replace(', ,',', ') for i in l]\n",
    "\n",
    "def extract_nouns(text):\n",
    "    return \" \".join([word \\\n",
    "                     for word,tag in nltk.pos_tag(text.split()) \\\n",
    "                     if(tag==\"NN\" or tag==\"NNP\" or tag==\"NNS\" or tag==\"NNPS\" or tag==\"FW\")])\n",
    "\n",
    "def extract_nouns_in_list(l):\n",
    "    df[\"noun_ing\"] = [extract_nouns(i) for i in l]\n",
    "    return df[\"noun_ing\"]\n",
    "    \n",
    "def re_process(a):\n",
    "    def re_p(text):\n",
    "        text = re.sub(r\"(.*)\\b(chicken|salt)\\b(.*)\",r\"\\2\",text)                                        #only keep keywords\n",
    "        text = re.sub(r\"(.*)\\b(leaves|large|fresh|shredded|\\\n",
    "        plain|crushed|medium|ground)\\b(.*)\",r\"\\1\\3\",text)  #remove common adjectives\n",
    "        #text = re.sub(r\"(.*)\\b(cheese|flour|milk|chilies|salt|oil\\\n",
    "        #|chicken|rice|wine|onion|beans|sugar)\\b(.*)\",r\"\\2\",text)    #only keep keywords\n",
    "        #text = re.sub(\"^(water|salt|pepper|oil|butter)$\",\"\",text)                               #remove common ingredients\n",
    "        #text = re.sub(r\"(.*)\\\n",
    "        #(ground|low fat|saturated|fresh|medium|flakes|low sodium|juice|dark|black|refried\\\n",
    "        #shredded|grated|extract|pitted|all-purpose|powder|juice|large|green|red|seedless\\\n",
    "        #blanched|sliced|crushed|wedgie|sharp|whole|wholesome|freshly|plain|and)\\\n",
    "        #\\s(.*)\",r\"\\1\\3\",text)                                                                   #remove common adjectives\n",
    "        text = re.sub(r\"(.*)(lime)(.*)\",r\"(\\1)(lemon)(\\2)\",text)                                #replace synonymes ? (lemmatize)\n",
    "        return text\n",
    "    def list_re(b):\n",
    "        return [re_p(i) for i in b]\n",
    "    def str_re(b):\n",
    "        return \",\".join([re_p(i) for i in b.split(\",\")])\n",
    "    if type(a[0])==list:\n",
    "        return a.apply(list_re)\n",
    "    else:\n",
    "        return a.apply(str_re)\n",
    "    \n",
    "def make_str_ing(a):\n",
    "    return a.apply(\",\".join)     #string ingredients\n",
    "    \n",
    "def make_und_ing(ing_list,ing_str=None):\n",
    "    if type(ing_str) != pd.core.series.Series:\n",
    "        ing_str = make_str_ing(ing_list)\n",
    "    return [i.replace(\" \",\"_\").replace(\",\",\" \") for i in ing_str]\n",
    "\n",
    "def make_dtm(a):\n",
    "    vect = CountVectorizer(input=\"content\",strip_accents=\"ascii\",binary=True)\n",
    "    vect.fit(list(a))\n",
    "    pickle.dump(vect,open(model_name+\"_vect.p\",\"wb\"))\n",
    "    return vect.transform(list(a))\n",
    "\n",
    "def stem(b,stemmer=None):\n",
    "    def stemm(a):\n",
    "        if type(a) == list:\n",
    "            return [stemmer.stem(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([stemmer.stem(i) for i in a.split(\",\")])\n",
    "    def lemmatizer(a):\n",
    "        if type(a) == list:\n",
    "            return [WordNetLemmatizer().lemmatize(i) for i in a]\n",
    "        else:\n",
    "            return \",\".join([WordNetLemmatizer().lemmatize(i) for i in a.split(\",\")])\n",
    "    if stemmer==None:\n",
    "        return b.apply(lemmatizer)\n",
    "    else:\n",
    "        return b.apply(stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################### Preprocessing ###############################\n",
    "df = pickle.load(open(\"df_bckup3.p\",\"rb\"))\n",
    "df[\"ing\"] = re_process(df[\"nouns_ing\"])\n",
    "#df[\"ing\"] = stem(df[\"ing\"],LancasterStemmer())\n",
    "df[\"ing\"] = clean(df[\"ing\"])\n",
    "df[\"ing\"] = remove_empty(df[\"ing\"])\n",
    "df[\"ing\"] = make_und_ing(df[\"ing\"])\n",
    "\n",
    "dtm = make_dtm(df[\"ing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### For full train data ##################################\n",
    "x_train = dtm\n",
    "y_train = np.array(df[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################### Split train data and cv data ############################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train,x_cv,y_train,y_cv = train_test_split(dtm,np.array(df[\"cuisine\"]),test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################### Setting Training and Testing Data #####################\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n",
    "def classify(model,name):\n",
    "    start_time = time()\n",
    "    model = Pipeline([('feature_selection',LinearSVC(penalty=\"l1\",dual=False)),('classification',model)])\n",
    "    model.fit(x_train,y_train)\n",
    "    acc = round(model.score(x_cv,y_cv),4)*100\n",
    "    end_time = round(time()-start_time,0)\n",
    "    print(name,acc,end_time)\n",
    "    with open(\"log_classifiers.txt\",\"a\") as text:\n",
    "        print(model_name+\",\"+name+\",\"+str(acc)+\",\"+str(end_time),file=text)\n",
    "    return acc,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM1 76.67 46.0\n",
      "SVM1 76.67\n"
     ]
    }
   ],
   "source": [
    "###################### Classifying single-iteration classifiers ####################\n",
    "clfs = [\n",
    "        (LinearSVC(loss='l2',penalty='l1',dual=False,tol=1e-3),\"SVM1\")\n",
    "        #,(LinearSVC(loss='l2',penalty='l2',dual=False,tol=1e-3),\"SVM2\")\n",
    "        #,(linear_model.LogisticRegression(C=1e5),\"Logistic Regression\")\n",
    "        ]\n",
    "\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a,c = classify(clf,name)\n",
    "    pickle.dump(c,open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "    if a > best_acc:\n",
    "        best_acc = a\n",
    "        best_name = name\n",
    "\n",
    "print(best_name,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################### Classifying multiple-iteration classifiers #################\n",
    "clfs = [\n",
    "        #(tree.DecisionTreeClassifier(),\"Decision Tree\")\n",
    "        #,(RandomForestClassifier(n_estimators=500,n_jobs=5),\"RF1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=3)),n_estimators=500,learning_rate=1),\"ADA 1\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=500,learning_rate=1),\"ADA 2\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1000,learning_rate=1),\"ADA 3\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=5)),n_estimators=1400,learning_rate=1),\"ADA 4\")\n",
    "        (AdaBoostClassifier((DecisionTreeClassifier(max_depth=7)),n_estimators=100,learning_rate=1),\"ADA 5\")\n",
    "        #,(AdaBoostClassifier((DecisionTreeClassifier(max_depth=9)),n_estimators=1200,learning_rate=1),\"ADA 6\")\n",
    "        ,(RandomForestClassifier(n_estimators=700,n_jobs=5),\"RF2\")\n",
    "        ]\n",
    "\n",
    "n = 8\n",
    "\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "for clf,name in clfs:\n",
    "    a = [0] * n\n",
    "    c = [0] * n\n",
    "    for i in range(0,n):\n",
    "        a[i],c[i] = classify(clf,name)\n",
    "        if a[i] > best_acc:\n",
    "            best_acc = a[i]\n",
    "            best_name = name\n",
    "    pickle.dump(c[a.index(max(a))],open(model_name+\"_\"+name+\".p\",\"wb\"))\n",
    "\n",
    "print(best_name,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>total</th>\n",
       "      <th>absolute false negatives</th>\n",
       "      <th>percentage false negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>brazilian</td>\n",
       "      <td>101</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>russian</td>\n",
       "      <td>99</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>208</td>\n",
       "      <td>102</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>irish</td>\n",
       "      <td>141</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>194</td>\n",
       "      <td>65</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>british</td>\n",
       "      <td>184</td>\n",
       "      <td>56</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>japanese</td>\n",
       "      <td>353</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>filipino</td>\n",
       "      <td>193</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>korean</td>\n",
       "      <td>238</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jamaican</td>\n",
       "      <td>139</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>moroccan</td>\n",
       "      <td>202</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greek</td>\n",
       "      <td>319</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>423</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thai</td>\n",
       "      <td>449</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>french</td>\n",
       "      <td>796</td>\n",
       "      <td>-22</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian</td>\n",
       "      <td>955</td>\n",
       "      <td>-42</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mexican</td>\n",
       "      <td>2049</td>\n",
       "      <td>-80</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chinese</td>\n",
       "      <td>829</td>\n",
       "      <td>-62</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>1430</td>\n",
       "      <td>-131</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>italian</td>\n",
       "      <td>2631</td>\n",
       "      <td>-307</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cuisine  total  absolute false negatives  percentage false negatives\n",
       "13     brazilian    101                        57                          56\n",
       "17       russian     99                        54                          55\n",
       "0        spanish    208                       102                          49\n",
       "16         irish    141                        61                          43\n",
       "18    vietnamese    194                        65                          34\n",
       "2        british    184                        56                          30\n",
       "14      japanese    353                        55                          16\n",
       "8       filipino    193                        28                          15\n",
       "12        korean    238                        33                          14\n",
       "10      jamaican    139                        16                          12\n",
       "11      moroccan    202                        22                          11\n",
       "4          greek    319                        33                          10\n",
       "3   cajun_creole    423                        43                          10\n",
       "9           thai    449                        19                           4\n",
       "5         french    796                       -22                          -3\n",
       "1         indian    955                       -42                          -4\n",
       "15       mexican   2049                       -80                          -4\n",
       "19       chinese    829                       -62                          -7\n",
       "6    southern_us   1430                      -131                          -9\n",
       "7        italian   2631                      -307                         -12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### Percentage errors in individual cuisines reults for cv #########\n",
    "from collections import Counter\n",
    "a=Counter(c.predict(x_cv))\n",
    "b=Counter(y_cv)\n",
    "d=pd.DataFrame([(i[0],i[1],b[i[0]] - i[1],round((b[i[0]] - i[1])/i[1]*100)) for i in a.items()]).sort_values(by=3,ascending=False)\n",
    "d.columns = [\"cuisine\",\"total\",\"absolute false negatives\",\"percentage false negatives\"]\n",
    "with open(\"error_cuisines.txt\",\"w\") as text:\n",
    "    print(d,file=text)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### make a data frame which shows similarity between cuisines ################\n",
    "common = []\n",
    "cuisines = set(y_cv)\n",
    "for i in cuisines:\n",
    "    for j in cuisines:\n",
    "        if i==j:\n",
    "            continue\n",
    "        i1 = len(list(y_cv[y_cv==i]))                          #Total tuples in original dataset\n",
    "        i2 = len((pred_cv[y_cv==i])[pred_cv[y_cv==i]==i])      #Total tuples in predicted\n",
    "        i3 = 100 - round(i2/i1 * 100)                          #% inaccuracy\n",
    "        i4 = len((pred_cv[y_cv==i])[pred_cv[y_cv==i]==j])      #Actually in cuisine 1 but predicted in cuisine 2\n",
    "        common.append([i,j,i1,i2,i3,i4])\n",
    "common = pd.DataFrame(common)\n",
    "common.columns = [\"cuisine 1\",\"cuisine 2\",\"total\",\"pred\",\"%\",\"v\"]\n",
    "common = common.sort_values(by=\"v\",ascending=False)\n",
    "with open(\"cuisines_similarity.txt\",\"w\") as text:\n",
    "    print(common,file=text)\n",
    "    \n",
    "common = pd.pivot_table(common,values = \"v\",\\\n",
    "                        columns=[\"cuisine 1\",\"total\",\"pred\",\"%\"], index = \"cuisine 2\")\n",
    "with open(\"cuisines_similarity.txt\",\"a\") as text:\n",
    "    print(common,file=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ Intro #####################################\n",
    "\n",
    "#clean : (oz.),keep alpha_num,lowercase\n",
    "#extract nouns\n",
    "#regex\n",
    "\n",
    "#cv : train_test_split - 30%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
